# =============================================================================
# Model Provider & Endpoint Configuration
# Loaded by ModelsProperties.kt and EndpointProperties.kt via Spring Boot.
# See docs/structures.md § "Ollama Instance Architecture" for hardware layout.
# =============================================================================

# -- LLM Providers ------------------------------------------------------------
# Each provider maps to one Ollama instance or cloud API.
# executionMode: INTERRUPTIBLE (can be preempted by user) | NONBLOCKING (runs freely)
# maxConcurrentRequests: Semaphore limit enforced by ModelConcurrencyManager.kt
providers:
  OLLAMA:
    endpoint: ollama.primary       # GPU instance (:11434) – Qwen 30B on P40
    executionMode: INTERRUPTIBLE   # Background tasks preempted by user chat
    maxConcurrentRequests: 1       # P40 handles 1 concurrent request efficiently

  OLLAMA_QUALIFIER:
    endpoint: ollama.qualifier     # CPU instance (:11435) – qwen2.5 7B/14B
    executionMode: NONBLOCKING     # Background ingest, no preemption needed
    # Matches OLLAMA_NUM_PARALLEL=10 on CPU instance and TaskQualificationService concurrency
    maxConcurrentRequests: 10

  OLLAMA_EMBEDDING:
    endpoint: ollama.embedding     # CPU instance (:11435) – qwen3-embedding:8b
    executionMode: NONBLOCKING
    # Embedding is fast (single forward pass), Ollama queues excess beyond NUM_PARALLEL
    maxConcurrentRequests: 50

  ANTHROPIC:
    endpoint: anthropic            # Claude API
    executionMode: NONBLOCKING
    maxConcurrentRequests: 3

  GOOGLE:
    endpoint: google               # Gemini API
    executionMode: NONBLOCKING
    maxConcurrentRequests: 3

  OPENAI:
    endpoint: openai               # OpenAI API
    executionMode: NONBLOCKING
    maxConcurrentRequests: 3

  LM_STUDIO:
    endpoint: lmStudio             # LM Studio (desktop dev)
    executionMode: NONBLOCKING
    maxConcurrentRequests: 25

# -- Model definitions ---------------------------------------------------------
# Per-type model list with provider binding. Used by ModelCandidateSelector.
models:
  EMBEDDING:
    - provider: OLLAMA_EMBEDDING
      model: qwen3-embedding:8b
      context-length: 32 768       # Max input tokens
      dimension: 4096              # Output vector dimension
      concurrency: 50              # Per-model semaphore (ModelConcurrencyManager)

# -- Endpoint URLs -------------------------------------------------------------
# Resolved by KtorClientFactory to create per-provider HTTP clients.
# ENV overrides (e.g., OLLAMA_BASE_URL) allow K8s/docker configuration.
endpoints:
  openai:
    apiKey: ${OPENAI_API_KEY}
    baseUrl: https://api.openai.com/v1/
  anthropic:
    apiKey: ${ANTHROPIC_API_KEY}
    baseUrl: https://api.anthropic.com/v1/
  google:
    apiKey: ${GOOGLE_API_KEY}
    baseUrl: https://generativelanguage.googleapis.com/v1beta/
  ollama:
    primary:
      baseUrl: ${OLLAMA_BASE_URL:http://192.168.100.117:11430/}            # Via Ollama Router
    qualifier:
      baseUrl: ${OLLAMA_QUALIFIER_BASE_URL:http://192.168.100.117:11430/}  # Via Ollama Router
    embedding:
      baseUrl: ${OLLAMA_EMBEDDING_BASE_URL:http://192.168.100.117:11430/}  # Via Ollama Router
  lmStudio:
    baseUrl: ${LMSTUDIO_BASE_URL:http://192.168.101.249:1234/}

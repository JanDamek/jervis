# Logging level configuration
logging:
  level:
    root: INFO
    com.jervis: INFO
    com.jervis.service: INFO
    org.springframework.web: INFO
    org.springframework.web.reactive.socket: INFO
    org.springframework.web.reactive.handler: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: ${DATA_ROOT_DIR}/logs/jervis-app.log
  logback:
    rollingpolicy:
      max-history: 7
      max-file-size: 10MB

spring:
  data:
    mongodb:
      host: 192.168.100.117
      port: 27017
      database: jervis
      # Uncomment and set these for production with authentication
      username: root
      password: qusre5-mYfpox-dikpef
      authentication-database: admin
      auto-index-creation: true

# Vector database configuration
# Weaviate - hybrid search vector DB with BM25 + semantic search
weaviate:
  enabled: true
  host: 192.168.100.117  # Change to TrueNAS host after deployment
  port: 8080  # HTTP REST API port
  scheme: http
  grpc-port: 50051  # gRPC port for high-performance operations
  hybrid-search:
    enabled: true
    alpha: 0.75  # 0.75 = 75% semantic + 25% keyword (BM25)

server:
  port: 5500
  ssl:
    enabled: true
    key-store: classpath:keystore.p12
    key-store-password: changeit
    key-store-type: PKCS12
    key-alias: jervis
  netty:
    connection-timeout: 600s # 10 minutes for long-running agent operations
    idle-timeout: 600s

management:
  endpoints:
    web:
      exposure:
        include: "health"
  endpoint:
    health:
      probes:
        enabled: true

jervis:
  background:
    enabled: true
    wait-on-error: 30s

models: # embedding dimensions configured per model
  EMBEDDING_TEXT:
    - provider: LM_STUDIO
      model: text-embedding-bge-m3  # Upgraded from nomic-embed-text-v2-moe for better multilingual support
      context-length: 4096  # Massively increased from 512 - allows much larger chunks
      dimension: 1024  # Increased from 768 for better semantic understanding
      concurrency: 8
  #    - provider: OLLAMA
  #      model: text-embedding-nomic-embed-text-v2-moe
  #      maxTokens: 512
  #      maxInputTokens: 512

  EMBEDDING_CODE:
    - provider: LM_STUDIO
      model: text-embedding-nomic-embed-code
      context-length: 4096
      dimension: 3584
      concurrency: 4
  #    - provider: OLLAMA
  #      model: nomic-embed-code
  #      maxTokens: 4096
  #      maxInputTokens: 4096

  QUESTION_INTERPRETER:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      num-predict: 65536
#      concurrency: 1

  CODER:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 132768
      num-predict: 4096
      quick: false
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      num-predict: 65536
#      concurrency: 1

  PLANNER:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 8192
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      num-predict: 65536
#      concurrency: 1

  JOERN:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
      concurrency: 1

  GENERIC_TEXT_MODEL:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      num-predict: 65536
#      concurrency: 1

  GENERIC_CODE_MODEL:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 8192
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      concurrency: 1
#      num-predict: 65536

  # Heavy models - BATCH PROCESSING ONLY due to P40 VRAM limitations
  HEAVY_TEXT_MODEL:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 8192
      quick: false
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      num-predict: 65536
#      concurrency: 1
#    - provider: OPENAI
#      model: gpt-4o
#      context-length: 16384
#      quick: false
#      concurrency: 1

  HEAVY_CODE_MODEL:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 8192
      quick: false
      concurrency: 1
#    - provider: GOOGLE
#      model: gemini-2.5-pro-latest
#      context-length: 1048576
#      num-predict: 65536
#      concurrency: 1

  QUICK:
    - provider: OLLAMA_QUALIFIER
      model: qwen2.5:3b-instruct-q4_K_M
      context-length: 8196
      quick: false
      concurrency: 2

  QUALIFIER:
    - provider: OLLAMA_QUALIFIER
      model: qwen2.5:3b-instruct-q4_K_M
      context-length: 32768
      concurrency: 2

endpoints:
  openai:
    api-key: ${OPENAI_API_KEY}
    base-url: https://api.openai.com/v1/
  anthropic:
    api-key: ${ANTHROPIC_API_KEY}
    base-url: https://api.anthropic.com/
  google:
    api-key: ${GOOGLE_API_KEY}
    base-url: https://generativelanguage.googleapis.com/
  ollama:
    primary:
      base-url: ${OLLAMA_BASE_URL:http://192.168.100.117:11434/}
    qualifier:
      base-url: ${OLLAMA_QUALIFIER_BASE_URL:http://192.168.100.117:11435/}
  lmStudio:
    base-url: ${LMSTUDIO_BASE_URL:http://192.168.101.249:1234/}
  searxng:
    base-url: ${SEARXNG_BASE_URL:http://192.168.100.117:30053/}
  joern:
    base-url: ${JOERN_BASE_URL:http://192.168.100.117:8082/}
  tika:
    base-url: ${TIKA_BASE_URL:http://192.168.100.117:8081/}
  whisper:
    base-url: ${WHISPER_BASE_URL:http://192.168.100.117:8083/}

# HTTP client connections and timeouts (used by WebClient)
connections:
  connection-pool:
    max-connections: 500
    max-idle-time: 30m
    max-life-time: 60m
    pending-acquire-timeout: 30m
    evict-in-background: 60m
    pending-acquire-max-count: 1000
  timeouts:
    connect-timeout-millis: 60000
    response-timeout-millis: 180000  # 3 minutes for embedding operations

# Retry configuration for external HTTP calls
retry:
  webclient:
    max-attempts: 5
    initial-backoff-millis: 1000
    max-backoff-millis: 30000
    backoff-multiplier: 2.0

# Workspace configuration - central root for all server-managed files
data:
  root-dir: ${DATA_ROOT_DIR}

# Audio processing configuration
audio:
  monitoring:
    default-git-check-interval-minutes: 15
    supported-formats:
      - wav
      - mp3
      - m4a
      - flac
      - ogg
      - opus
      - webm

# Text chunking configuration
text:
  chunking:
    max-tokens: 2048  # Increased to utilize bge-m3's 8192 token context window (with safety margin)
    overlap-percentage: 15  # Reduced overlap as chunks are now larger
    chars-per-token: 4.0

# Link indexing configuration
link:
  indexing:
    skip-if-indexed-within: 30d

# Git synchronization configuration
git:
  sync:
    # Interval for checking and syncing Git repositories (in milliseconds)
    polling-interval-ms: 300000 # 5 minutes
    initial-delay-ms: 60000 # 1 minute after startup

# Continuous indexing flow settings (global)
# jervis:
#   indexing:
#     flow:
#       buffer-size: 128  # Buffer between producer and processing; 0 disables buffering

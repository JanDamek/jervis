# Vision Augmentation Architecture

## ğŸ“‹ PÅ™ehled

**ProblÃ©m**: Apache Tika je slepÃ¡ - extrahuje text, ale nevidÃ­ **vÃ½znam** screenshotÅ¯, grafÅ¯, diagramÅ¯ a scannovanÃ½ch PDF.

**Å˜eÅ¡enÃ­**: Integrace **Qwen2.5-VL** (vision model) do Qualifier Agent jako **LLM node**, ne jako Tool.

---

## ğŸ¯ KlÃ­ÄovÃ© principy

### âŒ Co NEDÄšLAT (Anti-patterns)

```kotlin
// âŒ Å PATNÄš - Vision jako Tool
@Tool
fun analyzeAttachment(attachmentId: String): String {
    val model = selectVisionModel(...)
    return llmGateway.call(model, image) // LLM call v Toolu!
}
```

**ProÄ je to Å¡patnÄ›:**
- Tool API je pro **akce** (uloÅ¾it do DB, vytvoÅ™it task), ne pro LLM cally
- ZtrÃ¡cÃ­me type-safety Koog grafu
- Nelze vyuÅ¾Ã­t Koog multimodal (rÅ¯znÃ© modely per node)
- KomplikovanÃ© testovÃ¡nÃ­

### âœ… Co DÄšLAT (SprÃ¡vnÃ½ pÅ™Ã­stup)

```kotlin
// âœ… SPRÃVNÄš - Vision jako LLM Node v grafu
val nodeVisionAugmentation by nodeLLMRequest<ChunkWithContext, AugmentedChunk>(
    name = "Vision Augmentation",
    modelOverride = visionModel,  // Koog multimodal
    promptBuilder = { context ->
        Prompt.build("vision") {
            system("Analyze images...")
            user {
                text(context.chunkText)
                image(context.attachments[0].binaryData) // Koog image DSL
            }
        }
    }
)
```

**ProÄ je to sprÃ¡vnÄ›:**
- Vision je **LLM call**, proto je to **node v grafu**
- Koog multimodal: `modelOverride` per node
- Type-safe data flow: `ChunkWithContext â†’ AugmentedChunk`
- Conditional routing: `.onCondition { hasAttachments }`
- SnadnÃ© testovÃ¡nÃ­: node = pure function

---

## ğŸ—ï¸ Architektura (3 Layers)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: CONTINUOUS INDEXER                                     â”‚
â”‚  - Extract attachments from Jira/Confluence/Email               â”‚
â”‚  - Download binaries (images, PDFs)                             â”‚
â”‚  - Store in DirectoryStructureService                           â”‚
â”‚  - Attach metadata to PendingTask                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: QUALIFIER AGENT (CPU - Map/Reduce)                     â”‚
â”‚  Phase 1: SPLIT (semantic chunking)                             â”‚
â”‚  Phase 2: MAP (per-chunk processing)                            â”‚
â”‚    â”œâ”€ PrepareChunk: Text + attachments                          â”‚
â”‚    â”œâ”€ VisionAugmentation: Qwen2.5-VL analysis (conditional)     â”‚
â”‚    â””â”€ ExtractKnowledge: Store to RAG + Graph                    â”‚
â”‚  Phase 3: REDUCE (synthesis)                                    â”‚
â”‚  Phase 4: TASK CREATION                                         â”‚
â”‚  Phase 5: ROUTING                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: WORKFLOW AGENT (GPU - Deep Processing)                 â”‚
â”‚  - If LIFT_UP: Can re-analyze attachments with more powerful    â”‚
â”‚    vision model (e.g., qwen3-vl-tool-32k:latest on GPU)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Data Model

### PendingTaskDocument Extension

```kotlin
@Document(collection = "pending_tasks")
data class PendingTaskDocument(
    // ... existing fields

    /** Attachments for vision analysis */
    val attachments: List<AttachmentMetadata> = emptyList(),
)

@Serializable
data class AttachmentMetadata(
    val id: String,
    val filename: String,
    val mimeType: String,
    val sizeBytes: Long,
    val storagePath: String, // Relative to DirectoryStructureService
    val type: AttachmentType,
    val widthPixels: Int?, // For image token estimation
    val heightPixels: Int?, // For image token estimation
)

enum class AttachmentType {
    IMAGE,          // PNG, JPG, JPEG - high priority for vision
    PDF_SCANNED,    // PDF where Tika returned empty (needs OCR)
    PDF_STRUCTURED, // PDF where Tika got text (may still have charts)
    DOCUMENT,       // DOCX, XLSX, PPTX
    UNKNOWN,
}
```

### Qualifier Agent Data Flow

```kotlin
// Phase 2: MAP - Chunk processing with vision

data class ChunkWithContext(
    val chunkText: String,
    val chunkIndex: Int,
    val referencedAttachments: List<AttachmentData>, // Filtered for this chunk
)

data class AttachmentData(
    val id: String,
    val filename: String,
    val mimeType: String,
    val type: AttachmentType,
    val binaryData: ByteArray, // Loaded in memory
    val widthPixels: Int,
    val heightPixels: Int,
)

data class AugmentedChunk(
    val originalText: String,
    val visionDescriptions: List<VisionDescription>,
    val chunkIndex: Int,
) {
    fun toCombinedText(): String = buildString {
        append(originalText)
        if (visionDescriptions.isNotEmpty()) {
            append("\n\n## Visual Content Analysis\n\n")
            visionDescriptions.forEach { vision ->
                append("### ${vision.filename}\n")
                append(vision.description)
                append("\n\n")
            }
        }
    }
}

data class VisionDescription(
    val attachmentId: String,
    val filename: String,
    val model: String, // e.g., "qwen3-vl-tool-16k:latest"
    val description: String,
)
```

---

## ğŸ§© Smart Model Selection (Dynamic Context)

### Vision Model Selection with Token Estimation

```kotlin
@Service
class SmartModelSelector(
    private val tokenCountingService: TokenCountingService,
) {
    companion object {
        private val AVAILABLE_TIERS = listOf(4, 8, 16, 32, 40, 48, 64, 80, 96, 112, 128, 192, 256)
        private const val IMAGE_TOKEN_COMPRESSION_RATIO = 400
    }

    data class ImageMetadata(
        val widthPixels: Int,
        val heightPixels: Int,
    ) {
        fun estimateTokens(): Int {
            val pixels = widthPixels * heightPixels
            return (pixels / IMAGE_TOKEN_COMPRESSION_RATIO).coerceAtLeast(100)
        }
    }

    /**
     * Select vision model with dynamic context based on:
     * - Text prompt tokens
     * - Image resolution (pixels â†’ tokens)
     * - Output reserve
     */
    fun selectVisionModel(
        baseModelName: String,          // "qwen3-vl:latest"
        textPrompt: String,
        images: List<ImageMetadata>,
        outputReserve: Int = 2000,
    ): LLModel {
        val textTokens = tokenCountingService.countTokens(textPrompt)
        val imageTokens = images.sumOf { it.estimateTokens() }
        val totalTokensNeeded = textTokens + imageTokens + outputReserve

        val selectedTierK = AVAILABLE_TIERS.firstOrNull { (it * 1024) >= totalTokensNeeded }
            ?: AVAILABLE_TIERS.last()

        val modelId = insertTierIntoModelName(baseModelName, selectedTierK)
        // "qwen3-vl:latest" + 16k â†’ "qwen3-vl-tool-16k:latest"

        return LLModel(
            provider = LLMProvider.Ollama,
            id = modelId,
            capabilities = listOf(LLMCapability.Vision),
            contextLength = (selectedTierK * 1024).toLong(),
        )
    }
}
```

### Token Estimation Examples

| Image Resolution | Estimated Tokens | Selected Tier |
|------------------|------------------|---------------|
| 512Ã—512 (screenshot) | ~650 tokens | 4k |
| 1024Ã—1024 (chart) | ~2600 tokens | 4k-8k |
| 2048Ã—2048 (document scan) | ~10000 tokens | 16k |
| 4096Ã—4096 (high-res PDF page) | ~40000 tokens | 48k |

**Formula**: `tokens â‰ˆ (width Ã— height) / 400`

**Ollama Modelfiles** (uÅ¾ existujÃ­ - vygenerovÃ¡no build scriptem):
```modelfile
FROM qwen3-vl:latest
PARAMETER num_ctx 4096
PARAMETER temperature 0.1

FROM qwen3-vl:latest
PARAMETER num_ctx 8192
PARAMETER temperature 0.1

FROM qwen3-vl:latest
PARAMETER num_ctx 16384
PARAMETER temperature 0.1

# VÅ¡echny tiers: 4k, 8k, 16k, 32k, 40k, 48k, 64k, 80k, 96k, 112k, 128k, 192k, 256k
```

---

## ğŸ”„ Qualifier Agent Strategy (Phase 2 - MAP)

### Vision Node Integration

```kotlin
val subgraphProcessing by subgraph<ProcessingState, ProcessingState>(
    name = "ğŸ“‹ MAP Phase"
) {

    // Node 1: Prepare chunk + filter attachments
    val nodePrepareChunk by node<ProcessingState, ChunkWithContext>(
        name = "Prepare Chunk"
    ) { state ->
        val chunkText = state.nextChunk() ?: error("No chunk")

        // Filter attachments referenced in this chunk
        val referencedAttachments = state.attachments.filter { att ->
            chunkText.contains(att.filename, ignoreCase = true) ||
            chunkText.contains("[IMAGE]") ||
            chunkText.contains("screenshot", ignoreCase = true)
        }.map { metadata ->
            // Load binary from DirectoryStructureService
            val binary = directoryStructureService.readAttachment(metadata.storagePath)
            AttachmentData(
                id = metadata.id,
                filename = metadata.filename,
                mimeType = metadata.mimeType,
                type = metadata.type,
                binaryData = binary,
                widthPixels = metadata.widthPixels ?: 1024,
                heightPixels = metadata.heightPixels ?: 1024,
            )
        }

        ChunkWithContext(
            chunkText = chunkText,
            chunkIndex = state.currentIndex,
            referencedAttachments = referencedAttachments,
        )
    }

    // Node 2: Vision augmentation (MULTIMODAL - Qwen2.5-VL)
    val nodeVisionAugmentation by nodeLLMRequest<ChunkWithContext, AugmentedChunk>(
        name = "Vision Augmentation",
        modelSelector = { context ->
            // Dynamic model selection based on image resolution
            val images = context.referencedAttachments.map { att ->
                SmartModelSelector.ImageMetadata(
                    widthPixels = att.widthPixels,
                    heightPixels = att.heightPixels,
                )
            }

            val visionPrompt = buildVisionPrompt(context.chunkText)

            smartModelSelector.selectVisionModel(
                baseModelName = "qwen3-vl:latest",
                textPrompt = visionPrompt,
                images = images,
                outputReserve = 2000,
            )
        },
        promptBuilder = { context ->
            Prompt.build("vision-analysis") {
                system("""
                    You are a Vision Analysis Expert for knowledge extraction.

                    Analyze attached images and provide factual descriptions.

                    **Focus areas:**
                    - ERROR SCREENSHOTS: Extract exact error text, UI context, stack traces
                    - GRAPHS/CHARTS: Describe trends, metrics, conclusions
                    - FORMS/DOCUMENTS: Extract structured data (key-value pairs)
                    - DIAGRAMS: Describe components and relationships

                    **Output format:** Markdown with clear headings.
                    **Style:** Factual, no speculation.
                """.trimIndent())

                user {
                    text("""
                        Context from document:
                        ${context.chunkText}

                        Analyze the following ${context.referencedAttachments.size} attachment(s):
                    """.trimIndent())

                    // Koog multimodal: Add images
                    context.referencedAttachments.forEach { att ->
                        text("\n\n### ${att.filename}")
                        image(att.binaryData) // Koog DSL
                    }
                }
            }
        },
        responseParser = { response, context ->
            val visionDescriptions = context.referencedAttachments.map { att ->
                VisionDescription(
                    attachmentId = att.id,
                    filename = att.filename,
                    model = response.model ?: "qwen3-vl",
                    description = response.content,
                )
            }

            AugmentedChunk(
                originalText = context.chunkText,
                visionDescriptions = visionDescriptions,
                chunkIndex = context.chunkIndex,
            )
        }
    )

    // Node 3: Extract knowledge (MULTIMODAL - Back to text model)
    val nodeExtractKnowledge by nodeLLMRequest<AugmentedChunk, String>(
        name = "Extract Knowledge",
        // Uses default model from AIAgentConfig (qwen3-coder-tool:30b)
        promptBuilder = { augmentedChunk ->
            Prompt.build("analyze-chunk") {
                system(promptRepository.getSystemPrompt("ANALYZE_CHUNK"))
                user(augmentedChunk.toCombinedText()) // Text + vision
            }
        }
    )

    // Node 4: Execute storeKnowledge() tool
    val nodeExecuteTools by nodeExecuteTool(name = "Store Knowledge")

    // Node 5: Record result
    val nodeRecordResult by node<String, ProcessingState>("Record Result") { result ->
        val chunkResult = parseChunkResult(result)
        currentState.copy(
            processedResults = currentState.processedResults + chunkResult,
            currentIndex = currentState.currentIndex + 1,
        )
    }

    // EDGES
    edge(nodeStart forwardTo nodePrepareChunk)

    // Conditional: Vision only if attachments present
    edge(nodePrepareChunk forwardTo nodeVisionAugmentation)
        .onCondition { ctx -> ctx.referencedAttachments.isNotEmpty() }

    // Skip vision if no attachments
    edge(nodePrepareChunk forwardTo nodeExtractKnowledge)
        .onCondition { ctx -> ctx.referencedAttachments.isEmpty() }
        .transformInput { ctx ->
            AugmentedChunk(ctx.chunkText, emptyList(), ctx.chunkIndex)
        }

    edge(nodeVisionAugmentation forwardTo nodeExtractKnowledge)
    edge(nodeExtractKnowledge forwardTo nodeExecuteTools)
    edge(nodeExecuteTools forwardTo nodeRecordResult)

    // Loop back if more chunks
    edge(nodeRecordResult forwardTo nodePrepareChunk)
        .onCondition { state -> state.hasMoreChunks() }

    edge(nodeRecordResult forwardTo nodeFinish)
        .onCondition { state -> !state.hasMoreChunks() }
}
```

---

## ğŸ“ Vision Prompts (prompts.yaml)

```yaml
prompts:
  vision:
    error_screenshot:
      system: |
        You are a Vision Analysis Expert specializing in software error screenshots.

        **Task:** Extract all visible information from error screenshots.

        **Output:**
        - Exact error text (no paraphrasing)
        - Error codes and stack traces (if visible)
        - UI context (which button/form caused it)
        - User actions leading to error

        **Style:** Factual, precise, no speculation.

    graph_analysis:
      system: |
        You are a Data Visualization Analyst.

        **Task:** Extract insights from charts and graphs.

        **Output:**
        - Data trends (rising, falling, stable)
        - Key metrics and values
        - Time periods
        - Main conclusion (1-2 sentences)

        **Style:** Focus on data, ignore visual styling.

    form_extraction:
      system: |
        You are a Document OCR Specialist.

        **Task:** Extract structured data from forms and documents.

        **Output:**
        - Key-value pairs (field names and values)
        - Tables (as markdown)
        - Signatures and dates

        **Style:** Structured, machine-readable.
```

---

## ğŸš€ ImplementaÄnÃ­ kroky

### FÃ¡ze 1: Data Model (DONE)
- âœ… `AttachmentMetadata` data class
- âœ… `ChunkWithContext`, `AugmentedChunk`, `VisionDescription`
- âœ… `SmartModelSelector.selectVisionModel()`
- âœ… `SmartModelSelector.ImageMetadata` s token estimation

### FÃ¡ze 2: Indexer (TODO)
- [ ] Upravit `JiraContinuousIndexer` - stÃ¡hnout attachments
- [ ] Upravit `ConfluenceContinuousIndexer` - stÃ¡hnout obrÃ¡zky z pages
- [ ] Upravit `EmailContinuousIndexer` - stÃ¡hnout email attachments
- [ ] `DirectoryStructureService.storeAttachment()` - uloÅ¾it binÃ¡rnÃ­ data
- [ ] `PendingTaskService.createTask()` - pÅ™idat `attachments` parameter

### FÃ¡ze 3: Qualifier Agent (TODO)
- [ ] PÅ™idat `attachments` do `ProcessingState`
- [ ] Node `nodePrepareChunk` - filtrovat attachments per chunk
- [ ] Node `nodeVisionAugmentation` - Koog multimodal LLM call
- [ ] Conditional edges - skip vision pokud nejsou attachments
- [ ] Prompt templates v `prompts.yaml`

### FÃ¡ze 4: Testing (TODO)
- [ ] Unit test `SmartModelSelector.selectVisionModel()`
- [ ] Integration test vision node s mock Ollama
- [ ] E2E test Jira issue â†’ vision analysis â†’ knowledge graph

### FÃ¡ze 5: Documentation (TODO)
- [ ] Update `qualifier-agent-strategy.md`
- [ ] Update `koog-libraries.md` s multimodal pÅ™Ã­klady
- [ ] Create Ollama Modelfile examples pro vision tiers

---

## ğŸ¯ Benefits

1. **SystÃ©movÄ› ÄistÃ½ design** - Vision je LLM node, ne Tool hack
2. **Koog multimodal** - KaÅ¾dÃ½ node mÅ¯Å¾e mÃ­t jinÃ½ model (text vs vision)
3. **Dynamic context** - Context window se pÅ™izpÅ¯sobuje rozliÅ¡enÃ­ obrÃ¡zkÅ¯
4. **Type-safe** - Compile-time check data flow
5. **Conditional execution** - Vision bÄ›Å¾Ã­ jen kdyÅ¾ je potÅ™eba
6. **Backwards compatible** - ExistujÃ­cÃ­ flow funguje bez attachments
7. **TestovatelnÃ½** - Nodes jsou pure functions

---

## ğŸ“Š Vision Token Consumption Examples

### ScÃ©nÃ¡Å™ 1: Jira Bug Report
```
Chunk text: "Application crashes when clicking Save. See screenshot."
Attachment: screenshot.png (1920x1080)

Token calculation:
- Text: 200 tokens
- Image: (1920 Ã— 1080) / 400 = 5184 tokens
- Output reserve: 2000 tokens
- TOTAL: 7384 tokens â†’ Selected tier: 8k

Model: qwen3-vl-tool-8k:latest
```

### ScÃ©nÃ¡Å™ 2: Confluence Chart
```
Chunk text: "Q3 sales performance analysis. Chart below shows..."
Attachment: sales_chart.png (800x600)

Token calculation:
- Text: 150 tokens
- Image: (800 Ã— 600) / 400 = 1200 tokens
- Output reserve: 2000 tokens
- TOTAL: 3350 tokens â†’ Selected tier: 4k

Model: qwen3-vl-tool-4k:latest
```

### ScÃ©nÃ¡Å™ 3: Email with PDF
```
Chunk text: "Contract for review. Terms on page 3."
Attachment: contract.pdf (scanned, 2480x3508 per page)

Token calculation:
- Text: 100 tokens
- Image (1 page): (2480 Ã— 3508) / 400 = 21,764 tokens
- Output reserve: 2000 tokens
- TOTAL: 23,864 tokens â†’ Selected tier: 32k

Model: qwen3-vl-tool-32k:latest
```

---

## ğŸ”’ Safety & Limits

```yaml
jervis:
  vision:
    enabled: true
    max-attachments-per-chunk: 3  # Prevent token explosion
    max-resolution: 4096x4096      # Downscale larger images
    supported-formats:
      - image/png
      - image/jpeg
      - image/jpg
      - image/webp
      - application/pdf
    confidence-threshold: 0.7       # Trigger vision if Tika confidence < 0.7
```

---

## ğŸ“š References

- Koog Framework: `docs/koog-libraries.md`
- Qualifier Agent Strategy: `docs/qualifier-agent-strategy.md`
- Smart Model Selector: `backend/server/src/main/kotlin/com/jervis/koog/SmartModelSelector.kt`
- Qwen2-VL Model Card: https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct

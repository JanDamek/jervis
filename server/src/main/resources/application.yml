# Logging level configuration
logging:
  level:
    root: INFO
    com.jervis: INFO
    com.jervis.service: DEBUG
    org.springframework.web: INFO
    org.springframework.web.reactive.socket: INFO
    org.springframework.web.reactive.handler: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: ~/.jervis/logs/jervis-app.log
  logback:
    rollingpolicy:
      max-history: 7
      max-file-size: 10MB

spring:
  data:
    mongodb:
      host: 192.168.100.117
      port: 27017
      database: jervis
      # Uncomment and set these for production with authentication
      username: root
      password: REDACTED_MONGODB_PASSWORD
      authentication-database: admin
      auto-index-creation: true

# Qdrant vector database configuration
qdrant:
  host: 192.168.100.117
  port: 6334

server:
  port: 5500
  netty:
    connection-timeout: 600s # 10 minutes for long-running agent operations
    idle-timeout: 600s

tika:
  ocr:
    enabled: true
    languages:
      - eng
      - ces
      - spa
      - slk
      - fin
      - nor
      - dan
      - pol
      - deu
      - hun
    timeout-ms: 120000

models: # embedding dimensions configured per model
  EMBEDDING_TEXT:
    - provider: LM_STUDIO
      model: text-embedding-nomic-embed-text-v2-moe
      context-length: 512
      dimension: 768
  #    - provider: OLLAMA
  #      model: text-embedding-nomic-embed-text-v2-moe
  #      maxTokens: 512
  #      maxInputTokens: 512

  EMBEDDING_CODE:
    - provider: LM_STUDIO
      model: text-embedding-nomic-embed-code
      context-length: 4096
      dimension: 3584
  #    - provider: OLLAMA
  #      model: nomic-embed-code
  #      maxTokens: 4096
  #      maxInputTokens: 4096

  QUESTION_INTERPRETER:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 4096

  RAG:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 131072
      num-predict: 4096

  CODER:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 132768
      num-predict: 4096
      quick: false
    - provider: ANTHROPIC
      model: claude-3-5-sonnet-20241022
      context-length: 16384
      quick: false

  PLANNER:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 8192

  JOERN:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 4096

  GENERIC_TEXT_MODEL:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
    - provider: OPENAI
      model: gpt-4o-mini
      context-length: 8192
      quick: true

  GENERIC_CODE_MODEL:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 8192
    - provider: ANTHROPIC
      model: claude-3-5-haiku-20241022
      context-length: 8192
      quick: true

  # Heavy models - BATCH PROCESSING ONLY due to P40 VRAM limitations
  HEAVY_TEXT_MODEL:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 8192
      quick: false
    - provider: OPENAI
      model: gpt-4o
      context-length: 16384
      quick: false

  HEAVY_CODE_MODEL:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 8192
      quick: false
    - provider: ANTHROPIC
      model: claude-3-5-sonnet-20241022
      context-length: 32768
      quick: false
#    - provider: OPENAI
#      model: gpt-4o
#      maxTokens: 8196
#      quick: true
#    - provider: ANTHROPIC
#      model: claude-3-5-sonnet
#      maxTokens: 8196
#      quick: true

endpoints:
  openai:
    api-key: ${OPENAI_API_KEY:}
  anthropic:
    api-key: ${ANTHROPIC_API_KEY:}
  ollama:
    base-url: ${OLLAMA_BASE_URL:http://192.168.100.117:11434/}
  lmStudio:
    base-url: ${LMSTUDIO_BASE_URL:http://192.168.101.249:1234/}
  searxng:
    base-url: ${SEARXNG_BASE_URL:http://192.168.100.117:30053/}

# Service-specific timeout configurations
timeouts:
  joern:
    process-timeout-minutes: 30
    version-timeout-minutes: 30
    parse-timeout-minutes: 30
    script-timeout-minutes: 30
    scan-timeout-minutes: 30
    help-command-timeout-minutes: 30

  webclient:
    connect-timeout-seconds: 60
    pending-acquire-timeout-minutes: 30
    response-timeout-seconds: 120

  qdrant:
    operation-timeout-minutes: 30

  mcp:
    joern-tool-timeout-seconds: 1800
    terminal-tool-timeout-seconds: 1800

# Retry configuration for external HTTP calls
retry:
  webclient:
    max-attempts: 5
    initial-backoff-millis: 1000
    max-backoff-millis: 30000
    backoff-multiplier: 2.0

# Service-specific connection pool configurations
connection-pools:
  webclient:
    max-connections: 500
    pending-acquire-max-count: 1000
    evict-in-background-minutes: 60
    pending-acquire-timeout-minutes: 30
    max-life-time-minutes: 60
    max-idle-time-minutes: 30



# Workspace configuration - central root for all server-managed files
data:
  root-dir: ${DATA_ROOT_DIR:~/.jervis/workspace}

# Audio processing configuration
audio:
  monitoring:
    default-git-check-interval-minutes: 15
    supported-formats:
      - wav
      - mp3
      - m4a
      - flac
      - ogg
      - opus
      - webm

  whisper:
    enabled: true
    api-url: http://192.168.100.117:9000/
    timeout-seconds: 300
    max-retries: 3

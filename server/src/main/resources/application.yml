# Logging level configuration
logging:
  level:
    root: INFO
    com.jervis: INFO
    com.jervis.service: DEBUG
    org.springframework.web: INFO
    org.springframework.web.reactive.socket: INFO
    org.springframework.web.reactive.handler: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: ~/.jervis/logs/jervis-app.log
  logback:
    rollingpolicy:
      max-history: 7
      max-file-size: 10MB

spring:
  data:
    mongodb:
      host: 192.168.100.117
      port: 27017
      database: jervis
      # Uncomment and set these for production with authentication
      username: root
      password: REDACTED_MONGODB_PASSWORD
      authentication-database: admin
      auto-index-creation: true

# Qdrant vector database configuration
qdrant:
  host: 192.168.100.117
  port: 6334

server:
  port: 5500
  netty:
    connection-timeout: 600s # 10 minutes for long-running agent operations
    idle-timeout: 600s

management:
  endpoints:
    web:
      exposure:
        include: "health"
  endpoint:
    health:
      probes:
        enabled: true

models: # embedding dimensions configured per model
  EMBEDDING_TEXT:
    - provider: LM_STUDIO
      model: text-embedding-nomic-embed-text-v2-moe
      context-length: 512
      dimension: 768
  #    - provider: OLLAMA
  #      model: text-embedding-nomic-embed-text-v2-moe
  #      maxTokens: 512
  #      maxInputTokens: 512

  EMBEDDING_CODE:
    - provider: LM_STUDIO
      model: text-embedding-nomic-embed-code
      context-length: 4096
      dimension: 3584
  #    - provider: OLLAMA
  #      model: nomic-embed-code
  #      maxTokens: 4096
  #      maxInputTokens: 4096

  QUESTION_INTERPRETER:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
    - provider: GOOGLE
      model: gemini-2.5-pro-latest
      context-length: 1048576
      num-predict: 65536

  CODER:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 132768
      num-predict: 4096
      quick: false
    - provider: ANTHROPIC
      model: claude-3-5-sonnet-20241022
      context-length: 16384
      quick: false

  PLANNER:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 8192
    - provider: GOOGLE
      model: gemini-2.5-pro-latest
      context-length: 1048576
      num-predict: 65536

  JOERN:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
    - provider: GOOGLE
      model: gemini-2.5-pro-latest
      context-length: 1048576
      num-predict: 65536

  GENERIC_TEXT_MODEL:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 4096
    - provider: GOOGLE
      model: gemini-2.5-pro-latest
      context-length: 1048576
      num-predict: 65536
  #    - provider: OPENAI
  #      model: gpt-4o-mini
  #      context-length: 8192
  #      quick: true

  GENERIC_CODE_MODEL:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 8192
  #    - provider: ANTHROPIC
  #      model: claude-3-5-haiku-20241022
  #      context-length: 8192
  #      quick: true

  # Heavy models - BATCH PROCESSING ONLY due to P40 VRAM limitations
  HEAVY_TEXT_MODEL:
    - provider: OLLAMA
      model: qwen3:30b
      context-length: 32768
      num-predict: 8192
      quick: false
    - provider: GOOGLE
      model: gemini-2.5-pro-latest
      context-length: 1048576
      num-predict: 65536
    - provider: OPENAI
      model: gpt-4o
      context-length: 16384
      quick: false

  HEAVY_CODE_MODEL:
    - provider: OLLAMA
      model: qwen3-coder:30b
      context-length: 32768
      num-predict: 8192
      quick: false
    - provider: ANTHROPIC
      model: claude-3-5-sonnet-20241022
      context-length: 32768
      quick: false
  #    - provider: OPENAI
  #      model: gpt-4o
  #      maxTokens: 8196
  #      quick: true
  #    - provider: ANTHROPIC
  #      model: claude-3-5-sonnet
  #      maxTokens: 8196
  #      quick: true
  QUICK:
    - provider: OLLAMA_QUALIFIER
      model: qwen2.5:3b-instruct-q4_K_M
      context-length: 8196
      quick: false

  QUALIFIER:
    - provider: OLLAMA_QUALIFIER
      model: qwen2.5:3b-instruct-q4_K_M
      context-length: 32768
      concurrency: 4

endpoints:
  openai:
    api-key: ${OPENAI_API_KEY}
    base-url: ${OPENAI_BASE_URL:https://api.openai.com/v1/}
  anthropic:
    api-key: ${ANTHROPIC_API_KEY}
    base-url: ${ANTHROPIC_BASE_URL:https://api.anthropic.com/}
  google:
    api-key: ${GOOGLE_API_KEY}
    base-url: ${GOOGLE_BASE_URL:https://generativelanguage.googleapis.com/}
  ollama:
    primary:
      base-url: ${OLLAMA_BASE_URL:http://192.168.100.117:11434/}
    qualifier:
      base-url: ${OLLAMA_QUALIFIER_BASE_URL:http://192.168.100.117:11435/}
  lmStudio:
    base-url: ${LMSTUDIO_BASE_URL:http://192.168.101.249:1234/}
  searxng:
    base-url: ${SEARXNG_BASE_URL:http://192.168.100.117:30053/}
  joern:
    base-url: ${JOERN_BASE_URL:http://192.168.100.117:8082/}
  tika:
    base-url: ${TIKA_BASE_URL:http://192.168.100.117:8081/}
  whisper:
    base-url: ${WHISPER_BASE_URL:http://192.168.100.117:8083/}

# HTTP client connections and timeouts (used by WebClient)
connections:
  connection-pool:
    max-connections: 500
    max-idle-time: 30m
    max-life-time: 60m
    pending-acquire-timeout: 30m
    evict-in-background: 60m
    pending-acquire-max-count: 1000
  timeouts:
    connect-timeout-millis: 60000

# Retry configuration for external HTTP calls
retry:
  webclient:
    max-attempts: 5
    initial-backoff-millis: 1000
    max-backoff-millis: 30000
    backoff-multiplier: 2.0

# Workspace configuration - central root for all server-managed files
data:
  root-dir: ${DATA_ROOT_DIR:~/.jervis/workspace}

# Audio processing configuration
audio:
  monitoring:
    default-git-check-interval-minutes: 15
    supported-formats:
      - wav
      - mp3
      - m4a
      - flac
      - ogg
      - opus
      - webm

# Text chunking configuration
text:
  chunking:
    max-tokens: 400
    overlap-percentage: 10
    chars-per-token: 4.0

# Link indexing configuration
link:
  indexing:
    skip-if-indexed-within: 30d

# Git synchronization configuration
git:
  sync:
    # Interval for checking and syncing Git repositories (in milliseconds)
    polling-interval-ms: 300000 # 5 minutes
    initial-delay-ms: 60000 # 1 minute after startup